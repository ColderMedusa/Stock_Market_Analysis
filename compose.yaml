volumes:
    kafka_data: # Volume for Kafka message data persistence
    prometheus-data: # Volume for Prometheus data persistence
    spark-data: # Volume for Spark logs or data persistence
    pgdata:
    pgadmin_data: 
    grafana_storage: # Volume for Grafana data persistence
    ivy_cache: # Volume for Ivy cache (Maven dependencies)
    
networks:
    stock_data: # Custom network for Kafka and UI communication
#       #----------Kafka Configuration-----------
services:
    kafka:
        image: confluentinc/cp-kafka:7.4.10   # Use the Confluent Kafka image (version 7.4.10)
        container_name: kafka                 # Explicit container name for easier reference
        hostname: kafka                       # Hostname used inside Docker network
        ports:  
            - "9092:9092"                       # Internal listener for Docker containers
            - "9094:9094"                       # External listener for local host (Python clients)
        environment:
            KAFKA_KRAFT_MODE: "true"            # Enable KRaft mode (no Zookeeper)
            KAFKA_PROCESS_ROLES: controller,broker   # Kafka acts as both controller and broker
            KAFKA_NODE_ID: 1                    # Unique ID for this Kafka node
            KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093" # Define controller voters (only one here)
                
            KAFKA_LISTENERS: PLAINTEXT_INTERNAL://0.0.0.0:9092,PLAINTEXT_EXTERNAL://0.0.0.0:9094,CONTROLLER://0.0.0.0:9093
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT_INTERNAL://kafka:9092,PLAINTEXT_EXTERNAL://localhost:9094

            # Map listener names to security protocols
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT_INTERNAL:PLAINTEXT,PLAINTEXT_EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
            KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT_INTERNAL   # Brokers communicate via internal listener
            KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER            # Controller listener name
            KAFKA_LOG_DIRS: /var/lib/kafka/data       # Directory for Kafka logs (message data)
            KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"   # Allow automatic topic creation
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # Replication factor = 1 since single broker
            KAFKA_LOG_RETENTION_HOURS: 168            # Keep messages for 7 days
            KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0 # No delay before consumer group rebalance
            CLUSTER_ID: "jbzXiYiPQ8a2ejaWHK4Q3w"      # Unique cluster ID for KRaft mode 
      
        volumes:
            - kafka_data:/var/lib/kafka/data          # Persist Kafka message data

        networks:
            - stock_data                             # Attach Kafka to shared network for UI and Spark

#       #------------Kafka UI Configuration-----------
    kafka-ui: 
        container_name: kafka-ui # Set container name for Kafka UI
        image: "provectuslabs/kafka-ui:v0.7.2" # Use Provectus Kafka UI image
        ports:
            - "8085:8080"
        depends_on:
            - kafka
        environment:
            KAFKA_CLUSTERS_0_NAME: stock_data # Name to display for the Kafka cluster in the UI
            KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092 # Internal address of the Kafka broker for the UI to connect to
            DYNAMIC_CONFIG_ENAABLED: 'true' # Enable dynamic configuration in Kafka UI
        networks:
            - stock_data # Attach UI to the same UI as kafka so it can communicate with the broker


#       #-----------GRAFANA CONFIGURATION-----------
#services:
    grafana:
      image: grafana/grafana:12.3.2-security-01
      container_name: grafana
      restart: unless-stopped
      environment:
        # increases the log level from info to debug
        - GF_LOG_LEVEL=debug
        #- GF_SECURITY_ADMIN_PASSWORD=admin
        - GF_AUTH_ANONYMOUS_ENABLED=true
      ports:
        - '3000:3000'
      volumes:
        - 'grafana_storage:/var/lib/grafana'
## volumes:
##   grafana_storage: {}

#services:
    prometheus:
        image: prom/prometheus:v2.50.0
        container_name: prometheus
        ports:
          - "9090:9090"
        volumes:
          - prometheus-data:/prometheus
        command:
          - "--config.file=/etc/prometheus/prometheus.yml"
          - "--storage.tsdb.retention.time=15d"
        restart: unless-stopped

#----------SPARK MASTER CONFIGURATION-----------
#services:
    spark-master:
        image: apache/spark:4.0.2-python3               # Use the official Apache Spark image with Python 3 support (v3.5.7)
        container_name: spark-master             # Assign a fixed, human-readable name to this container
        command: [                           # Override the default container command to start Spark master manually
          "/opt/spark/bin/spark-class", 
          "org.apache.spark.deploy.master.Master",  
          "--host", "spark-master",                    
          "--port", "7077",                            
          "--webui-port", "8080"]    # Set the hostname that other nodes use to reach the master
        environment:
          - SPARK_MODE=master                    # (Optional variable) indicates this container acts as master
          - SPARK_MASTER_HOST=spark-master       # Hostname of the Spark master, used by workers to locate it
        ports:
          - "8081:8080"                          # Expose master web UI (container 8080) on host port 8081
          - "7077:7077"                          # Expose Spark master service port for worker/job connections
        volumes:
          - spark-data:/opt/spark-data           # Mount a named volume to persist Spark logs or data
        networks:
          - stock_data                          # Connect the container to a shared Docker network for inter-service communication

# -----------SPARK WORKER CONFIGURATION-----------
    spark-worker:
        image: apache/spark:4.0.2-python3               # Use the same Apache Spark image for the worker (v3.5.7)            
        container_name: spark-worker             # Name for the worker container
        command: [                           # Override the default container command to start Spark master manually
          "/opt/spark/bin/spark-class", 
          "org.apache.spark.deploy.worker.Worker",  
          "spark://spark-master:7077",                           
          "--webui-port", "8081"] 
        environment:
          - SPARK_WORKER_CORES=2                 # Allocate 2 CPU cores to this worker
          - SPARK_WORKER_MEMORY=2G               # Allocate 2 GB of memory to this worker
        depends_on:
          - spark-master                         # Ensure Spark master starts before the worker tries to connect
        volumes:
          - spark-data:/opt/spark-data           # Share the same named volume for logs/data if needed
        networks:
          - stock_data                          # Attach to same network so it can communicate with the master

    consumer:
      build:
        context: ./consumer
        dockerfile: Dockerfile
      image: stream_consumer
      container_name: consumer
      depends_on:
        - spark-master
        - kafka
      env_file: .env
      restart: unless-stopped
      volumes:
        - ivy_cache:/opt/spark/work-dir/.ivy2
        - ./consumer:/opt/spark/work-dir/apps
      networks:
        - stock_data          


#services:
    postgres:
        image: postgres:14.21-trixie
        container_name: postgres-db
        environment:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: secret
          POSTGRES_DB: myapp
        ports:
          - "5432:5432"
        volumes:
          - pgdata:/var/lib/postgresql/data

    pgadmin:
        image: dpage/pgadmin4:9.12.0
        container_name: pgadmin-web
        environment:
          PGADMIN_DEFAULT_EMAIL: admin@example.com
          PGADMIN_DEFAULT_PASSWORD: admin
        ports:
          - "5050:80"
        depends_on:
          - postgres
        volumes:
          - pgadmin_data:/var/lib/pgadmin
## volumes:
# #    pgdata:
#  #   pgadmin_data: